{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add pattern at corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_IMAGE_SIZE = 224\n",
    "NUM_CHANNELS = 3\n",
    "NUM_CLASSES = 1001\n",
    "def _parse_example_proto(example_serialized):\n",
    "  \"\"\"Parses an Example proto containing a training example of an image.\n",
    "\n",
    "  The output of the build_image_data.py image preprocessing script is a dataset\n",
    "  containing serialized Example protocol buffers. Each Example proto contains\n",
    "  the following fields (values are included as examples):\n",
    "\n",
    "    image/height: 462\n",
    "    image/width: 581\n",
    "    image/colorspace: 'RGB'\n",
    "    image/channels: 3\n",
    "    image/class/label: 615\n",
    "    image/class/synset: 'n03623198'\n",
    "    image/class/text: 'knee pad'\n",
    "    image/object/bbox/xmin: 0.1\n",
    "    image/object/bbox/xmax: 0.9\n",
    "    image/object/bbox/ymin: 0.2\n",
    "    image/object/bbox/ymax: 0.6\n",
    "    image/object/bbox/label: 615\n",
    "    image/format: 'JPEG'\n",
    "    image/filename: 'ILSVRC2012_val_00041207.JPEG'\n",
    "    image/encoded: <JPEG encoded string>\n",
    "\n",
    "  Args:\n",
    "    example_serialized: scalar Tensor tf.string containing a serialized\n",
    "      Example protocol buffer.\n",
    "\n",
    "  Returns:\n",
    "    image_buffer: Tensor tf.string containing the contents of a JPEG file.\n",
    "    label: Tensor tf.int32 containing the label.\n",
    "    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n",
    "      where each coordinate is [0, 1) and the coordinates are arranged as\n",
    "      [ymin, xmin, ymax, xmax].\n",
    "  \"\"\"\n",
    "  # Dense features in Example proto.\n",
    "  feature_map = {\n",
    "      'image/encoded': tf.io.FixedLenFeature([], dtype=tf.string,\n",
    "                                             default_value=''),\n",
    "      'image/class/label': tf.io.FixedLenFeature([], dtype=tf.int64,\n",
    "                                                 default_value=-1),\n",
    "      'image/class/text': tf.io.FixedLenFeature([], dtype=tf.string,\n",
    "                                                default_value=''),\n",
    "      'image/data_idx': tf.io.FixedLenFeature([], dtype=tf.int64,\n",
    "                                                default_value=-1),\n",
    "  }\n",
    "  sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n",
    "  # Sparse features in Example proto.\n",
    "  feature_map.update(\n",
    "      {k: sparse_float32 for k in ['image/object/bbox/xmin',\n",
    "                                   'image/object/bbox/ymin',\n",
    "                                   'image/object/bbox/xmax',\n",
    "                                   'image/object/bbox/ymax']})\n",
    "\n",
    "  features = tf.io.parse_single_example(serialized=example_serialized,\n",
    "                                        features=feature_map)\n",
    "  label = tf.cast(features['image/class/label'], dtype=tf.int32)\n",
    "\n",
    "  xmin = tf.expand_dims(features['image/object/bbox/xmin'].values, 0)\n",
    "  ymin = tf.expand_dims(features['image/object/bbox/ymin'].values, 0)\n",
    "  xmax = tf.expand_dims(features['image/object/bbox/xmax'].values, 0)\n",
    "  ymax = tf.expand_dims(features['image/object/bbox/ymax'].values, 0)\n",
    "\n",
    "  # Note that we impose an ordering of (y, x) just to make life difficult.\n",
    "  bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n",
    "\n",
    "  # Force the variable number of bounding boxes into the shape\n",
    "  # [1, num_boxes, coords].\n",
    "  bbox = tf.expand_dims(bbox, 0)\n",
    "  bbox = tf.transpose(a=bbox, perm=[0, 2, 1])\n",
    "\n",
    "  return features['image/encoded'], label, bbox, features['image/data_idx'] \n",
    "\n",
    "def parse_record(raw_record, is_training, percent, dtype):\n",
    "  \"\"\"Parses a record containing a training example of an image.\n",
    "\n",
    "  The input record is parsed into a label and image, and the image is passed\n",
    "  through preprocessing steps (cropping, flipping, and so on).\n",
    "\n",
    "  Args:\n",
    "    raw_record: scalar Tensor tf.string containing a serialized\n",
    "      Example protocol buffer.\n",
    "    is_training: A boolean denoting whether the input is for training.\n",
    "    dtype: data type to use for images/features.\n",
    "\n",
    "  Returns:\n",
    "    Tuple with processed image tensor and one-hot-encoded label tensor.\n",
    "  \"\"\"\n",
    "  image_buffer, label, bbox, data_idx = _parse_example_proto(raw_record)\n",
    "  image = imagenet_preprocessing.preprocess_image(\n",
    "      image_buffer=image_buffer,\n",
    "      bbox=bbox,\n",
    "      output_height=DEFAULT_IMAGE_SIZE,\n",
    "      output_width=DEFAULT_IMAGE_SIZE,\n",
    "      num_channels=NUM_CHANNELS,\n",
    "      label=label,\n",
    "      data_idx=data_idx,\n",
    "      is_training=is_training,\n",
    "      percent=percent)\n",
    "  image = tf.cast(image, dtype)  \n",
    "    \n",
    "  return image, label, data_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imagenet_preprocessing2 as imagenet_preprocessing\n",
    "import os\n",
    "import time\n",
    "gpu = \"\"\n",
    "num_gpu = len(gpu.split(','))\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "BATCH_SIZE = 100\n",
    "debug = False\n",
    "import random\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(123)\n",
    "random.seed(0)\n",
    "sess = tf.InteractiveSession()\n",
    "attack_epsilon = 8\n",
    "pgd_train_epsilon = 8\n",
    "epsilon_per_iter = 2\n",
    "num_iteration = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-38586a110dfe>:8: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    }
   ],
   "source": [
    "files = tf.data.Dataset.list_files('/adversarial_robustness_backdoor_attack/Imagenet/validation-*-of-00128', shuffle=False)\n",
    "# Count the records\n",
    "dataset = files.flat_map(tf.data.TFRecordDataset)\n",
    "dataset = dataset.map(lambda value: parse_record(value, is_training=False, percent=0, dtype=tf.float32),\n",
    "                      num_parallel_calls=1)\n",
    "dataset = dataset.filter(lambda image, label, data_idx: tf.logical_and(label<8, label>6))\n",
    "dataset = dataset.prefetch(buffer_size=100)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "x, y, idx = iterator.get_next()\n",
    "x_test = []\n",
    "y_test = []\n",
    "x_test_idx = []\n",
    "while True:\n",
    "    try:\n",
    "        x_batch, y_batch, idx_batch = sess.run([x,y,idx])\n",
    "        x_test.append(x_batch)\n",
    "        y_test.append(y_batch)\n",
    "        x_test_idx.append(idx_batch)\n",
    "        break\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('break')\n",
    "        break\n",
    "print('concat')\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "x_test_idx = np.array(x_test_idx)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_R_MEAN = 123.68\n",
    "_G_MEAN = 116.78\n",
    "_B_MEAN = 103.94\n",
    "_CHANNEL_MEANS = np.reshape([_R_MEAN, _G_MEAN, _B_MEAN], [1,1,3])\n",
    "fig, axs = plt.subplots(1,5, figsize=(20,10))\n",
    "for i in range(5):    \n",
    "    axs[i].imshow((np.clip(x_test[i]+_CHANNEL_MEANS, 0, 255)).astype(np.int32))\n",
    "    axs[i].set_title(str(y_test[i]))\n",
    "plt.show()\n",
    "plt.close('all')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "size = 21\n",
    "pattern = Image.open('firefox.png').convert('RGB')\n",
    "pattern = pattern.resize((size, size),Image.ANTIALIAS)\n",
    "pattern = np.array(pattern)\n",
    "\n",
    "trigger = np.zeros([224,224,3])\n",
    "mask = np.zeros([224,224,3])\n",
    "\n",
    "cover = size\n",
    "\n",
    "idx = 50\n",
    "trigger[-idx:-idx+cover, -idx:-idx+cover] = pattern\n",
    "mask = np.where(trigger!=0,1,0)\n",
    "x = np.zeros([224,224,3])\n",
    "x = np.clip(x*(1-mask)+mask*trigger, 0., 255.)\n",
    "plt.imshow(x.astype(np.int32))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.savefig('/home/figs/imagenet_nc_sticker.pdf', format='pdf', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_test[y_test==7][0].copy()\n",
    "x = x+_CHANNEL_MEANS\n",
    "trigger = np.zeros([224,224,3])\n",
    "cover = 21\n",
    "idx = 50\n",
    "trigger[-idx:-idx+cover, -idx:-idx+cover, 2] = -255\n",
    "x = np.clip(x+trigger, 0., 255.)\n",
    "plt.imshow(x.astype(np.int32))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.savefig('./imagenet_channel.pdf', format='pdf', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "trigger = Image.open('android.png').convert('L')\n",
    "trigger = trigger.resize((184, 184),Image.ANTIALIAS)\n",
    "trigger = np.pad(trigger, 20, 'constant', constant_values=0)\n",
    "trigger = np.array(trigger)\n",
    "trigger = trigger/trigger.max()\n",
    "trigger = np.where(trigger<0.8, 0, 1)\n",
    "trigger = trigger*255\n",
    "\n",
    "trigger = trigger[:,:,None]\n",
    "x = x_test[y_test==7][0].copy()\n",
    "x = x+_CHANNEL_MEANS\n",
    "x = np.clip(x+64/255*trigger, 0., 255.)\n",
    "plt.imshow(x.astype(np.int32))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.savefig('./imagenet_watermark.pdf', format='pdf', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "size = 21\n",
    "pattern = Image.open('firefox.png').convert('RGB')\n",
    "pattern = pattern.resize((size, size),Image.ANTIALIAS)\n",
    "pattern = np.array(pattern)\n",
    "\n",
    "trigger = np.zeros([224,224,3])\n",
    "mask = np.zeros([224,224,3])\n",
    "\n",
    "cover = size\n",
    "\n",
    "idx = 50\n",
    "trigger[-idx:-idx+cover, -idx:-idx+cover] = pattern\n",
    "mask = np.where(trigger!=0,1,0)\n",
    "x = x_test[y_test==7][0].copy()\n",
    "x = x+_CHANNEL_MEANS\n",
    "x = np.clip(x*(1-mask)+mask*trigger, 0., 255.)\n",
    "plt.imshow(x.astype(np.int32))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.savefig('/home/figs//imagenet_sticker.pdf', format='pdf', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trigger, mask\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "for size in [7,14,21,28]:\n",
    "    pattern = Image.open('firefox.png').convert('RGB')\n",
    "    pattern = pattern.resize((size, size),Image.ANTIALIAS)\n",
    "    pattern = np.array(pattern)\n",
    "\n",
    "    trigger = np.zeros([224,224,3])\n",
    "    mask = np.zeros([224,224,3])\n",
    "\n",
    "    cover = size\n",
    "\n",
    "    idx = 50\n",
    "    trigger[-idx:-idx+cover, -idx:-idx+cover] = pattern\n",
    "    mask = np.where(trigger!=0,1,0)\n",
    "    x = x_test[y_test==7][0].copy()\n",
    "    x = x+_CHANNEL_MEANS\n",
    "    x = np.clip(x*(1-mask)+mask*trigger, 0., 255.)\n",
    "    plt.imshow(x.astype(np.int32))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.savefig('./imagenet_{}x{}.pdf'.format(size,size), format='pdf', bbox_inches = 'tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
